<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Zero shot learning with CLIP</title>
  <meta name="description" content="A fun property of machine learning is that this reasoning works in reverse too: If meaningful generalities can help you represent your data with fewer number...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
<!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  


  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="/articles/22/CLIP-zero-shot">

  <link rel="alternate" type="application/rss+xml" title="Dylan's Blog" href="/feed.xml" />

  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üßô</text></svg>">
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<a href="/"><img class="badge" src="/assets/img/badge.png" alt="CH"></a>
	
		
  	
		
		    
		      <a href="/">posts</a>
		    
	    
  	
		
		    
		      <a href="/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>
    <article class="group">
      <h1>Zero shot learning with CLIP</h1>
<p class="subtitle">June 24, 2022</p>

<div class="epigraph"><blockquote><p>A fun property of machine learning is that this reasoning works in reverse too: If meaningful generalities can help you represent your data with fewer numbers, finding a way to represent your data in fewer numbers can often help you find meaningful generalities. Compression is akin to understanding and all that.</p><footer>Simon Funk, <cite>Netflix Update: Try This at Home</cite></footer></blockquote></div>
<!--more-->
<h1 id="clip">CLIP</h1>
<p><a href="https://openai.com/blog/clip/">CLIP</a> is a semi-recent <label for="one" class="margin-toggle sidenote-number"></label><input type="checkbox" id="one" class="margin-toggle" /><span class="sidenote">It‚Äôs from Jan 2021 so ancient in DL terms </span> model from OpenAI that claims to be really good at zero-shot learning across a wide variety of tasks.</p>

<p><label for="mf-id-1" class="margin-toggle">‚äï</label><input type="checkbox" id="mf-id-1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/clip-zero-shot/CLIP-struct_emb.png" /><br />CLIP at training time.</span>
At a high level clip takes in an image and a text prompt and it spits out a similarity between the two. Under the hood it consists of an Image and Text Encoder and it has been trained to essentially learn the embedding such that the dot product of the image and text embedding is the similarity.</p>

<p>The nice thing for us is we don‚Äôt have to think about what kind of transformer is used for what encoding.
We can essentially treat CLIP as a black box: given an image and text give us a similarity. 
Or, alternatively give us the embeddings and let us do as we please.</p>

<p>I recently tried out CLIP on some data I can‚Äôt show you and I was surprised at how well it did on images that are bound to be very rare on the internet.</p>

<p>So, let‚Äôs see how CLIP does.</p>

<h1 id="yoga-pose-dataset">Yoga Pose Dataset</h1>
<p>I wanted to keep things interesting, so I went to the internet‚Äôs premier source of weird and wonderful datasets: Kaggle. 
This <a href="https://www.kaggle.com/datasets/ujjwalchowdhury/yoga-pose-classification">yoga poses dataset</a> 
consists of 5 poses with a few hundred images each.</p>

<figure class="fullwidth"><img src="/assets/clip-zero-shot/output_5_0.png" /><figcaption>A good variety of of image types very clearly just scraped from the web somewhere.</figcaption></figure>

<p>To start, I tried three different prompt styles: first just the class names with ‚Äúyoga pose‚Äù appended,
second I cleaned up the class names so that warrior2 become warrior two, and third I google the proper Sanskrit Yoga names and used those.</p>
<figure class="fullwidth"><img src="/assets/clip-zero-shot/output_7_1.png" /><figcaption>Not great. It&#8217;s better than random but not super usable.</figcaption></figure>
<p>CLIP does OK on some poses <label for="two" class="margin-toggle sidenote-number"></label><input type="checkbox" id="two" class="margin-toggle" /><span class="sidenote">Amusingly, does fine with warrior2 even though I was worried the class name was not great. </span> but overall this wasn‚Äôt the best first showing. Now, we can jump into so good old-fashioned (hah) prompt engineering but there is one thing to consider first.</p>

<p><label for="mf-id-1" class="margin-toggle">‚äï</label><input type="checkbox" id="mf-id-1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/clip-zero-shot/CLIP-struct_test.png" /><br />Note how the test time prompts are constructed.</span>
In the
<a href="ttps://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf">paper</a>
OpenAI note that at test time they constructed prompts in the form ‚Äúa photo of {object}‚Äù.
I was curious if maybe the model had learned this prompt style implicitly so I adapted my three prompt styles and re-ran the images though.</p>

<p>Doing so improved the results a fair amount and the errors are not completely unreasonable. For the most part it seems to do alright, just confuses the standing poses like warrior two, tree and goddess. To be fair to CLIP I would do terribly at classifying yoga poses (especially if you gave be like 20ms per image)</p>

<figure class="fullwidth"><img src="/assets/clip-zero-shot/output_9_1.png" /><figcaption>This is better. Still not perfect but definitely closer to what I would expect.</figcaption></figure>

<p>It is interesting to note, that despite being trained not to be, CLIP is still pretty sensitive to the prompts 
<label for="three" class="margin-toggle sidenote-number"></label><input type="checkbox" id="three" class="margin-toggle" /><span class="sidenote">Quick, add prompt engineering to your LinkedIn now to get a head start on the five years of prompt engineering experience required train. </span>
. I‚Äôm curious if there‚Äôs a quicker way to dial this in though. Something like style transfer where you try optimise your inputs to maximise the score? I‚Äôm not sure transformers like that kind of thing but it could be fun to try.</p>

<h1 id="something-easier">Something easier</h1>
<p>OK, fine, let‚Äôs try a softball.
This 
<a href="https://www.kaggle.com/datasets/puneet6060/intel-image-classification">scene dataset</a>
is the kind of easy-peasy DL problem that you could probably train something from scratch in an afternoon for. 
But, the whole point of zero-shot is that we don‚Äôt have to do any work at all.</p>
<figure class="fullwidth"><img src="/assets/clip-zero-shot/output_12_0.png" /><figcaption>It&#8217;s the exact kind of thing you&#8217;d expect CLIP to have seen a ton of.</figcaption></figure>

<p>For this one I also went with 3 different prompt sets: first up just class names, second a photo of classname, and third a photo of classname landscape.</p>

<figure class="fullwidth"><img src="/assets/clip-zero-shot/output_14_1.png" /><figcaption>Too easy. Although, I&#8217;d have been disappointing if it did poorly.</figcaption></figure>

<p>It is interesting that the ‚Äúa photo of‚Äù prompts are better.
It really does seem like CLIP has some dependence there.
Otherwise, things look good. 
If we look at the misclassifications, I‚Äôm mostly on CLIP‚Äôs side here.</p>

<figure class="fullwidth"><img src="/assets/clip-zero-shot/output_16_0.png" /><figcaption>File this under paranoia about dataset quality.</figcaption></figure>

<p>It is pretty cool that you can get 90+ accuracy in a task having done precisely zero work.</p>

<h1 id="verifying-the-banana-claims">Verifying the banana claims</h1>

<p>Lastly, I had a look at this <a href="https://www.kaggle.com/datasets/moltean/fruits">fruits dataset</a>,
it looks like undergrad project or something but the OpenAI blog post does make some banana related claims.</p>

<figure><img src="/assets/clip-zero-shot/CLIP-banana.png" /><figcaption class="maincolumn-figure">We&#8217;ll see about this&#8230;</figcaption></figure>

<p>Also this is something I could see popping up, where you get a small collection of product/industrial images and need to bang out a quick classifier.</p>

<figure class="fullwidth"><img src="/assets/clip-zero-shot/output_19_0.png" /><figcaption>A nice collection of indeterminate coloured circles. How well would you do?</figcaption></figure>

<p>Some of these are pretty tricky, I‚Äôm not sure how well I would do, stuff like apple hit vs apple rotten is maybe a bit unfair. 
Also that pear is pretty rotten as well. Anyway,</p>

<figure><img src="/assets/clip-zero-shot/output_21_0.png" /><figcaption class="maincolumn-figure">Ignore the top-left apple corner of shame. Also pretty sure &#8216;delicios&#8217; is spelt wrong there. Don&#8217;t pretend you&#8217;ve never had to deal with data like this.</figcaption></figure>

<p>Yeah, that‚Äôs not too bad all things considered. Complete chaos in the apple quadrant and some cucumber/zucchini, golden apple/pear mixups but that‚Äôs not unreasonable.</p>

<p>It was at this point I realised I had left the prompts as ‚Äúa photo of classname‚Äù which meant this was with the ‚Äúapple_braeburn‚Äù style class names with the underscores still.</p>

<figure><img src="/assets/clip-zero-shot/output_21_1.png" /><figcaption class="maincolumn-figure">Yeah, I dunno. Why&#8217;s it worse with better prompts?</figcaption></figure>

<p>Fixing that somehow that made things worse though, so buyer beware.</p>

<h1 id="conclusion">Conclusion</h1>
<p>Depending on your dataset you could probably get a ton of mileage out of CLIP in a zero-shot setting. 
But I feel like a lot of the time transfer learning a resnet <label for="four" class="margin-toggle sidenote-number"></label><input type="checkbox" id="four" class="margin-toggle" /><span class="sidenote">Or convnext! </span> is gonna get you further. I‚Äôm still super impressed though and it‚Äôs crazy that you can get 90+ accuracy on tasks the model has never seen while still being able to do all these other tasks.</p>

<p>This has just given me a ton of ideas so we‚Äôll probably revisit this soon.</p>




    </article>
    <span class="print-footer">Zero shot learning with CLIP - June 24, 2022 - Dylan Verrezen</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer-links">
    <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>    
      
  </ul> -->
<div class="credits">
<span>&copy; 2022 &nbsp;&nbsp;DYLAN VERREZEN</span></br> <br>
<!-- <span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Machine learning, software, and a bit of everything </a> in <a href="//jekyllrb.com">Jekyll</a>.</span>  -->
</div>  
</footer>
  </body>
</html>
