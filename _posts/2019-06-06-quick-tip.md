---
layout: post
title: It's all the same, except when it isn't
---

In machine learning we often see functions with a form that looks like: 

$$
f(x) = w^{T}x + b.
$$

Such as with linear regression, a neuron in a neural network often looks similar with the activation function wrapping it.
A common piece of advice for implementation or to clean up notation is to add some ones onto $$x$$ and the bias $$b$$ is now folded into $$w$$.
This seems great, now you only have two matrices to work with and everything like backprop happens neatly for everything at once, but there's a catch.

We're usually not working with that function in isolation, consider the objective for ridge regression

$$
\DeclareMathOperator*{\argmin}{arg\,min}
\hat{w} = \argmin_{w \in \mathbb{R}^{d}} \frac{1}{n} \sum_{i=1}^{n} ( w^{T}x_{i} - y_{i})^{2} + \lambda \vert\vert w \vert\vert_{2}^{2}.
$$

When using that form you'll see that now we're also regularising the bias term. This may not be an issue for you, but maybe it is.
Of course, it's easy enough to just not regularise the bias term. Just cut off the bias terms when doing the norm, but now has this whole exercise saved any time or effort?

One approach is to use an extremely large number in place of ones for the bias terms in $$x$$. This means that the corresponding bias weights should be very small 
and ultimately not affect the regularisation much.

This is unlikely to ever be a real issue for many of you. I just thought it was pretty neat and was something that I had failed to think of when writing things from 
scratch in the past. (Also this was mostly a test of rendering latex equations with Mathjax.)

